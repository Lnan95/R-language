---
title: "R for Data Science读书笔记"
author: "Linsinan"
Organization: "ZUEL"
date: "2016年12月25日"

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#一、工作环境
```{r,message=FALSE}
library(tidyverse)#tidyverse：数据整理工具的合集
```

```{r,eval=FALSE}
tidyverse_update()#更新此包
devtools::session_info()#环境查看
devtools::session_info(c("tidyverse"))#版本查询+环境查看
#数据集合
install.packages(c("nycflights13", "gapminder", "Lahman"))
```




#二、可视化(ggplot2)
```{r mpg}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

```

To plot mpg, run this code to put displ on the x-axis and hwy on the y-axis:

*displ*, 以升为单位的车的引擎大小

*hwy*,车在高速公路上的油耗，一加仑能跑多少里。

结论：A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.


```{r,eval=FALSE}
ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))
```


*ggplot(data = mpg)* 画一个空的图，*geom_point()* 为你的画增加一个图层，本函数专指散点图
每个geom函数都有一个mapping参数，这个参数定义了数据集中多少个变量将被用进可视化中
mapping经常与aes()一起出现, aes()中的x和y指定了x和y的相互匹配




###1.Aesthetic mappings
将点颜色按class列的变量匹配到每一辆车，同时增加一个legend
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))
```




也可以将点的大小按class列的变量匹配到每一辆车，不过不建议用size在离散变量中
```{r,warning=F}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, size = class))
```




我们还可以将class属性匹配到alpha或shape参数，他们分别控制了点的透明度和形状
```{r,warning=F}
# Left
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))



# Right
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, shape = class))

```

但是由于ggplot2一次只能使用6个形状，所以SUV变量不见了 




当然我们也可以手动设置绘图参数，比如把所有的点都涂蓝
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
```



当手动设置参数时，需要把参数放到aes()的外面，系统不会自动生产legend等信息，只单单成为蓝色。但，当我们尝试将*color="blue"*放到aes()中时，我们发现：

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
```

我们会发现，ggplot将"blue"的字符转换为了一种单一的颜色，图中为淡的红色，且还生产legend，显然不符合我们所需要的图形。


####1.1补充信息
fill表示点中间的颜色（制造出外黑内白的效果），stroke参数表示黑的部分的大小，即width of border
```{r}
ggplot(data = mpg) +
  geom_point(aes(displ, hwy),shape = 21, colour = "black", fill = "white", size = 4, stroke = 5)
```


displ<5的变量将赋予其他颜色（原理是不同的displ生产了对应true和false）
```{r}
ggplot(data = mpg) +
    geom_point(aes(displ, hwy,colour = displ < 5))
    #可在geom_point中设置参数show.legend = F，关闭legend
```


###2.分面 Facets
分面，即将你的分类变量按每一类画到一个图上
注意，进行分面的数据要选择离散的,数据连续的话每一个不同的数据都为作为一类。
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
```


如果想画两个变量组合的分面，可以使用facet_grid()函数，facet_grid()函数的第一个参数也是一个公式（formula），比如我们想做drv和cyl的组合分面，我们可以使用drv~cyl作为公式

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
```

值得注意的是，为facet\_grid(. ~ cyl)时和facet\_wrap(~ cyl, nrow=1)效果一样，当函数为facet\_grid(drv ~ .)时，等于图像向右旋转90°。


###3.几何模型Geometric objects
```{r,message=F}
ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy))
```

当我们使用其他geom\_xx函数时，我们可以获得不同的几何图形（geom=geometric）,但是我们要注意，每个geom\_xx函数都有mapping参数，但是不是每个aes()内的参数都一样的，比如线图中就没有shape参数，不过你可以使用linetype来设置每条线的类型。

```{r,message=F}
ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
```
geom_smooth()根据分类变量drv将数据分为了3条不同的线，每条线描述着不同的drv变量。本图中，三条线分别描述了drv=4,f和r时的mapping = aes(x = displ, y = hwy)的线。

我们将散点图和线图一起看时，你就能更明白这个意思
```{r,message=F}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = drv)) +
  geom_smooth(mapping = aes(x = displ, y = hwy, color = drv, linetype = drv))
```

我们还可以使用group变量进行最简单的分组
```{r,message=F}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))
```

也可以多个图层叠加，我们可以把mapping放到主函数ggplot中，这样可以使代码简约，当然我们还是可以在geom函数中增加参数，不过有的参数别忘了加到mapping=aes()中。
```{r,message=F}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth()
```


```{r,message=F}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)#se=F，表示不绘制区间
```

（filter函数下一章节会讲，我们知道可以使用filter只绘制出class为subcompact的平滑估计曲线就好）

我们再与下面2个图形对比，我们可以知道很明显发现彼此的区别,我们可以把aes中的参数理解为类似group by的操作。
```{r,message=F}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + 
  geom_point() + 
  geom_smooth(se = FALSE)

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth(se = FALSE)
```

更多的geom函数，可以查看：

[Rstudio](http://rstudio.com/cheatsheets)

<http://rstudio.com/cheatsheets>

[ggplot2](https://www.ggplot2-exts.org)

<https://www.ggplot2-exts.org>


###4.统计变化Statistical transformations
本次我们使用的是ggplot中的数据集diamonds了来绘制柱状图。
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut))
```

x轴上为cut,是diamonds数据集中的一个变量。y轴为计数值，也就是不同的cut在数据集中对应的数量。这就是bar函数的特别之处,因为它用统计变化制作出了新的变量：
比如：

    1.bar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin.

    2.smoothers fit a model to your data and then plot predictions from the model.

    3.boxplots compute a robust summary of the distribution and then display a specially formatted box.

这些获得新的数值并使用到图形上的算法就称为stat
比如，在geom\_bar中就应用了统计变换中的stat\_count函数，我们也可以直接用stat\_count绘制柱状图
```{r}
ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))
```

有几种情况你可以选择调整统计变换的方法：
1.当数据已经汇总好的时候，我们就需要调整统计变化的方法了，比如stat = "identity"：
```{r}
(demo <- tribble(
  ~a,      ~b,
  "bar_1", 20,
  "bar_2", 30,
  "bar_3", 40
))

ggplot(data = demo) +
  geom_bar(mapping = aes(x = a, y = b), stat = "identity")
```

因为此时如果使用count，那每个就只有1个了，而且x和y都赋值了，所以不修改stat变量函数会出错。（不理解可以对比上一张图的）

2.你想用柱状图表示其中每个变量的比例时
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = ..prop.., group = T))#group为1时表示其占整体的比例
```


3.如果想要汇总对应不同x值的y值，我们可以使用stat_summary()，本例中,生成的图像y的最小值为对应x值的min(depth)，最大值为max(depth)，点为median(depth)
```{r}
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )
```


###5.位置调整


在bar图中，我们可以使用fill来进行分组
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity))
```

每一块都自动地按照fill中的参数进行位置调整，如果你不想每块叠在一起，你可以使用"identity", "dodge" 或 "fill"进行调整，当然我们也可以调整geom_bar里面的填充和透明度。
position = "identity" 则不做统计变换，每个柱重叠了，所以y坐标对应值明显不同于count的
```{r}
ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 1/5)
ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 1/5, position = "identity")
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + 
  geom_bar(fill = NA, position = "identity")
```

position = "fill"时，和默认一样进行块的堆积，但是每个柱的高度会一样高，所以用来比较每个组(cut)中，clarity类型的比例。
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")
```

position = "dodge" 时，对每个clarity类型进行单独陈列
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
```

另外一种调整适合用在散点图中，相同数值的点会被覆盖掉，使用position="jitter"，可以对数据加上一点小小的扰动，使图形不被覆盖
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
```

ggplot中有更简洁的调用方法：
```{r,eval=FALSE}
ggplot(data = mpg) +
    geom_jitter(mapping = aes(x = displ, y = hwy))

```

同样还有position\_dodge,position\_fill, position\_identity, position\_jitter, 和 position\_stack
我们还可以这样子作图：
```{r,message==F}
p <- ggplot(mpg, aes(class, hwy))
p + geom_boxplot()
p + geom_boxplot() + geom_jitter(width = 0.2)
p + geom_boxplot() + coord_flip()
p + geom_boxplot(notch = TRUE)
p + geom_boxplot(varwidth = TRUE)
p + geom_boxplot(fill = "white", colour = "#3366FF")
```

还可以利用identity直接用分位数和中位数等信息绘制box
```{r}
y <- rnorm(100)
df <- data.frame(
  x = 1,
  y0 = min(y),
  y25 = quantile(y, 0.25),
  y50 = median(y),
  y75 = quantile(y, 0.75),
  y100 = max(y)
)
ggplot(df, aes(x)) +
  geom_boxplot(
    aes(ymin = y0, lower = y25, middle = y50, upper = y75, ymax = y100),
    stat = "identity"
  )
```

###6.坐标轴Coordinate systems
coord_flip()可以用于转换x和y轴
```{r}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot()
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()
```

coord_quickmap()可以绘制地图，可以用来分析空间数据,需要加载maps包
```{r,message=FALSE}
clibrary(maps)
nz <- map_data("nz")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
```

用coord_polar()画花冠图
```{r}
bar <- ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1#柱子宽，将左右柱子链接到一起,如果画玫瑰图，可以稍微重叠(width=1.1)，好看些
  ) + 
  theme(aspect.ratio = 1) +  #长宽比变1:1
  labs(x = NULL, y = NULL)#去掉x和y上的标签，一般为变量的名称，本例中卫count和cut

bar + coord_flip()
bar + coord_polar()#玫瑰图
```

p=ggplot(data = mpg, mapping = aes(x = cty, y = hwy))
p+geom_point()
p+geom_point()+geom_abline()+coord_fixed()
```{r}
p=ggplot(data = mpg, mapping = aes(x = cty, y = hwy))
p+geom_point()
p+geom_point()+geom_abline()+coord_fixed()#我们还可以通过geom_abline的intercept和slope调整截距和斜率
```

画辅助线的详细信息
<http://docs.ggplot2.org/current/geom_abline.html#>
我们还可以用coord_fixed调整比例

*按Alt + Shift + K可以查看rstudio的菜单*

###Data transformation


调用数据集和工作环境
```{r}
library(nycflights13)
library(tidyverse)
```

flights数据集2013年336,776个从纽约出发的航班信
```{r}
flights
```

这是一个tibble数据集


    int 表示整型数据

    dbl 表示double.

    chr 表示字符型.

    dttm 表示时间数据(a date + a time).

    lgl 表示逻辑变量，即 TRUE or FALSE.

    fctr 表示因子变量

    date 表示日期数据


###dplyr的基础操作

    filter()：按值选择变量 .
    
    arrange()：行的重新排列 .
    
    select()：按名字选择变量.
    
    mutate()：使用现有变量变换生成新变量 .
    
    summarise()：Collapse many values down to a single summary .

These can all be used in conjunction with *group_by()* which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation.

All verbs work similarly:
The first argument is a data frame.The subsequent arguments describe what to do with the data frame, using the variable names (without quotes).

The result is a new data frame.

Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let’s dive in and see how these verbs work.

###Filter rows with filter()
```{r}
filter(flights, month == 1, day == 1)
```
使用(jan1 <- filter(flights, month == 1, day == 1))加一个括号，可以即调用又打印

R中的双等号是不准确的。之所以sqrt(2) ^ 2=2，是因为小数点的省略(option)
```{r,eval=FALSE}
sqrt(2) ^ 2
#> [1] 2
sqrt(2) ^ 2 == 2
#> [1] FALSE
1/49 * 49 == 1
#> [1] FALSE
```

我们可以使用near函数
```{r,eval=FALSE}
sqrt(2) ^ 2-0.0000001
#> [1] 2
near(sqrt(2) ^ 2-0.0000001,  2)
#[1] FALSE
near(sqrt(2) ^ 2,  2)
#[1] TRUE
```

可见精度还是可以的。

###逻辑操作符Logical operators

1.& is “and”, | is “or”, and ! is “not”，值得注意的是
```{r,eval=FALSE}
near(!5, 0)
#[1] TRUE
```

```
如果你想选择起飞和着陆延迟都不超过2小时的数据，可以用这两个方法
filter(flights, !(arr_delay > 120 | dep_delay > 120))
filter(flights, arr_delay <= 120, dep_delay <= 120)
```

2. %in% 
判断是否存在的操作符
```
a = c(1:12)

a中含有2或者3的元素
a %in% c(2,3)
#[1] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE

c(2,3)是否在a中
c(2,3) %in% a
#[1] TRUE TRUE

看来是一个个匹配的↓，不能把1和3看作一个连续的整体
c(1,3) %in% a
#[1] TRUE TRUE
```

选择month列中含有11或12的行
```{r}
(nov_dec <- filter(flights, month %in% c(11, 12)))
```


###Missing values
```
NA > 5
#> [1] NA
10 == NA
#> [1] NA
NA + 10
#> [1] NA
NA / 2
#> [1] NA
```

The most confusing result is this one:

```
NA == NA
#> [1] NA
is.na(x)
#> [1] TRUE

filter()函数中，FALSE和na都被过滤
df <- tibble(x = c(1, NA, 3))
filter(df, is.na(x) | x > 1) 这样就可以保留
#> # A tibble: 2 × 1
#>       x
#>   <dbl>
#> 1    NA
#> 2     3
```

###Arrange
```{r}
arrange(flights, year, month, day)#依次year, month和day排序，从低到高，year一样比较month..
arrange(flights, desc(arr_delay))#desc()，降序
#缺失值永远排到最后
df <- tibble(x = c(5, 2, NA))
arrange(df, x)
arrange(df, desc(x))
```


###Select
```{r}
# 按列变量名称选择变量
select(flights, year, month, day)
# 可以用:的方法省略中间的列名
select(flights, year:day)
```


其他在select()中有用的函数(其实还是正则好用些):

    starts_with("abc"): 匹配开头是 “abc”的列.
    类似于flights[grep("^ye", names(flights))]
```{r}
    select(flights, starts_with("ye"))
```
   
ends_with("xyz"): 匹配结尾是 “xyz”的列.
类似于flights[grep("r$", names(flights))]
```{r}
    select(flights, ends_with("r"))
```
   
contains("ijk"):匹配包含 “ijk”的列.
类似于flights[grep("our", names(flights))]
```{r}
select(flights, contains("our"))
select(flights, contains("TIME"))#不区分大小写
```

    matches("(.)\\1"): 用正则表达式寻找变量

```{r}
select(flights, matches("our"))
select(flights, matches("r$"))
select(flights, matches("^ye"))
```

    num_range("x", 1:3) 匹配变量名为x1、x2和x3的列.

```{r}
df <- tibble(x1 = c(5, 2, NA),x2=c(4,5,6),x3=c(1,3,5))
select(df, num_range("x", 1:2))
#占位符好好用
select(df, matches(".x."))
select(df, matches("x."))
#！！！可以用负号去掉
select(df, -matches("x1"))
```

利用everything()重新排序
```{r}
select(df,x3,x2, everything())
```

用于重命名
```{r}
select(df, y = starts_with("x"))#如果选定的是多个，那y后加数字
#还可以
rename(df, y = x1)#不好用
```

one_of(),选择向量名字一样的列
```{r}
vars <- c("x1", "x2")
select(df, one_of(vars))
```

###用mutate()添加新变量

```{r}
#选择变量：year month day dep_delay arr_delay distance air_time
flights_sml <- select(flights, 
       year:day, 
       ends_with("delay"), 
       distance, 
       air_time
)

#mutate的生成的两个新变量都是在最后
mutate(flights_sml,
  gain = arr_delay - dep_delay,
  speed = distance / air_time * 60
)
```

还可以直接调用刚生产的变量
```{r}
mutate(flights_sml,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

如果你只想保留新生成的变量，用transmute():
```{r}
transmute(flights,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```


其他的一些操作
```
  1.算术操作符：+-*/^,如果两个向量长短不一，短的会自动补齐，常用的有，air_time / 60, 
  hours * 60 + minute这类的变换
  
  2.整合函数的链接，计算占比x / sum(x) ;计算与均值偏差y - mean(y) 
  
  3.模运算：  整除→%/%；取余数→%%；
    transmute(flights,
    dep_time,
    hour = dep_time %/% 60,
    minute = dep_time %% 60
    )
  
  4.log函数,log(), log2(),log10(),在处理跨越多个数量级的数据时很有用。
  log()函数中默认参数base=exp(1),作者推荐log2，因为易于解释，
  
  a difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving.
  
  我的理解是：在log变化后每变化1各单位，相当于原始数据与-1的差的两倍????
  大致就是会缩小变化程度的意思吧

```

```{r}
f=function(x) return(x^2+1)
df<-tibble(x=c(1:8),y=f(x),logx=log(x),logy=log(y))#也能调用刚生成的变量

p1 <- ggplot(data = df) + 
  geom_line(mapping = aes(x = x, y = y),color="navyblue") +
  geom_line(mapping = aes(x = x, y = logy),color='red') +
  annotate("text", label = c("y == x^2 +1","log[2](y) == x^2 + 1"), x = c(4,6), y = c(25,7.5), size = 5, colour = c("navyblue","red"),parse=T)#annotate是用来加文字的，label为文字，如是表达式，parse=T

p2 <- ggplot(data = df) + 
  geom_line(mapping = aes(x = logx, y = logy),color="orange") +
  annotate("text", label = "log[2](y) == log[2](x^2) + 1", x = 1, y = 2.5, size = 5, colour = "orange",parse=T)

gridExtra::grid.arrange(p1,p2,ncol=2) #利用gridExtra包合并图片
```

```
    5.lead()和lag()，将数据前移和后移，可以用于做差
    
    6.累计计算：cumsum(), cumprod(), cummin, cummax,cummean分别累加,累承，累计最小最大值，累计均值
    
    7.dplyr中的排序：
      min_rank 返回从小到大的index（加-号就变大到小了）；
      desc 将输入值从大到小排序
      row_number 从小到大排序，如果数据有结，最前面的结排序最高（同rank(ties.method = "first")）
      min_rank:从小到大排序，如果数据有结，则返回index可并列 rank(ties.method = "min")
      percent_rank 把排序映射到[0,1]中
      cume_dist 返回x比向量中百分之几的元素大（包括自己）
      ntile 将数据分为n份
```
```{r}

set.seed(101)
(a=rnorm(10))
min_rank(a)
min_rank(-a)
desc(a)

row_number(a)
rank(a,ties.method = "first")
rank(a)
rank(a,ties.method = "min")

percent_rank(a)
ntile(a,3)
ntile(a,2)
cume_dist(a)
```

###获得汇总数据：summarise()

```{r}
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
```


group_by函数与summarise的搭配！！！
使得可以求出每日的mean(delay)
```{r}
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

管道操作符
```{r}
by_dest <- group_by(flights, dest)
delay <- summarise(by_dest,
  count = n(),#计数
  dist = mean(distance, na.rm = TRUE),
  delay = mean(arr_delay, na.rm = TRUE)
)
delay <- filter(delay, count > 20, dest != "HNL")#删掉<=20的

# It looks like delays increase with distance up to ~750 miles 
# and then decrease. Maybe as flights get longer there's more 
# ability to make up delays in the air?
ggplot(data = delay, mapping = aes(x = dist, y = delay)) +
  geom_point(aes(size = count), alpha = 1/3) +
  geom_smooth(se = FALSE)
```

There are three steps to prepare this data:

    Group flights by destination.

    Summarise to compute distance, average delay, and number of flights.

    Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport.

管道函数：x %>% f(y) 相当于 f(x, y),  x %>% f(y) %>% g(z) 相当于 g(f(x, y),
```{r}
delays <- flights %>% 
  group_by(dest) %>% 
  summarise(
    count = n(),
    dist = mean(distance, na.rm = TRUE),
    delay = mean(arr_delay, na.rm = TRUE)
  ) %>% 
  filter(count > 20, dest != "HNL")
```


NA处理
```{r}
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))

not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(mean = mean(dep_delay))
```

计数count
计算非NA的个数sum(!is.na(x))
```{r}
#计数和分组1搭配使用！
delays <- not_cancelled %>% 
  group_by(tailnum) %>% 
  summarise(
    delay = mean(arr_delay),
    n = n()
  )

#惊了，有的航班平均延迟超过5小时
ggplot(data = delays, mapping = aes(x = delay)) + 
  geom_freqpoly(binwidth = 10)#密度图

#我们可以发现，次数少的航班在延迟上方差大
ggplot(data = delays, mapping = aes(x = n, y = delay)) + 
  geom_point(alpha = 1/10)

#基于管道直接处理并绘图！！ 
delays %>% 
  filter(n > 25) %>% 
  ggplot(mapping = aes(x = n, y = delay)) + 
    geom_point(alpha = 1/10)

```



```
这部分没明白，是用Ctrl + Shift + P重新读什么？

RStudio tip: a useful keyboard shortcut is Cmd/Ctrl + Shift + P. This resends the previously sent chunk from the editor to the console. This is very convenient when you’re (e.g.) exploring the value of n in the example above. You send the whole block once with Cmd/Ctrl + Enter, then you modify the value of n and press Cmd/Ctrl + Shift + P to resend the complete block.

```

棒球球员表现分析:
measured by the batting average, ba

```{r}
# Convert to a tibble so it prints nicely
batting <- as.tbl(Lahman::Batting)

batters <- batting %>% 
  group_by(playerID) %>% 
  summarise(
    ba = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),
    ab = sum(AB, na.rm = TRUE)
  )

batters %>% 
  filter(ab > 100) %>% 
  ggplot(mapping = aes(x = ab, y = ba)) +
    geom_point() + 
    geom_smooth(se = FALSE)

batters %>% 
  arrange(desc(ab))
```


利用逻辑变量，计算平均延时情况和正延迟时的平均延迟情况：

```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(
    avg_delay1 = mean(arr_delay),
    avg_delay2 = mean(arr_delay[arr_delay > 0]) # the average positive delay
  )
```

IQR() 计算4分位差(IQR which is simpler but less robust than mad)

mad() Median Absolute Deviation!
公式：constant * cMedian(abs(x - center))
center = median(x)(默认)；constant = 1.4826(一个修正，保证E[mad(X\_1,…,X\_n)] = σ)
```{r}
set.seed(101)
a <- rnorm(100*1000,sd = 3.14,mean = 1)
dim(a)<-c(100,1000)

a <- a %>%
    tbl_df() %>%
    apply(2,mad)  %>%
    mean()

#与方差的偏差
abs(3.14-a)

```

计算距离标准差
```{r}
not_cancelled %>% 
  group_by(dest) %>% 
  summarise(distance_sd = sd(distance)) %>% 
  arrange(desc(distance_sd))
```

位置函数
first(x), nth(x, 2), last(x)



These functions are complementary to filtering on ranks. Filtering gives you all variables, with each observation in a separate row:
不是很理解！！！

```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  mutate(r = min_rank(desc(dep_time))) %>% #min_rank(desc(dep_time))得到从大到小排列的index
  filter(r %in% range(r))                  #相当于求unique
```


计数！  
n_distinct(x) 相当于length(unique(x))
也可以使用count直接计数
```{r}
count(not_cancelled,month)
```

关于count的权重参数，其实就是计数后乘以对应的均值
```{r}
batting %>% count(playerID, wt = G) %>% arrange(playerID)

batting %>% group_by(playerID) %>% 
mutate(mea=mean(G),dc=n(),coun = mea*dc) %>%
select(starts_with("p"),mea,dc,coun) %>% 
arrange(playerID)

#使用sort=T,由大到小排列
batting %>% count(playerID, wt = G, sort = TRUE)
```

逻辑向量的计数或比例有时也会有用：

```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(n_early = sum(dep_time < 500))

# What proportion of flights are delayed by more than an hour?
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(hour_perc = mean(arr_delay > 60))
```

###分组
```{r}
daily <- group_by(flights, year, month, day)
(per_day <- summarise(daily, flights = n()))

#解除分组
daily %>% 
  ungroup() %>%
  summarise(flights = n())
```


#EDA(Exploratory Data Analysis)



    1.Generate questions about your data.

    2.Search for answers by visualising, transforming, and modelling your data.

    3.Use what you learn to refine your questions and/or generate new questions.

EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will hone in on a few particularly productive areas that you’ll eventually write up and communicate to others.



"There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox"

"Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey"


remenber the two question:
What type of variation occurs within my variables?

What type of covariation occurs between my variables?


可视化：
How you visualise the distribution of a variable will depend on whether the variable is categorical or continuous. A variable is categorical if it can only take one of a small set of values. In R, categorical variables are usually saved as factors or character vectors. To examine the distribution of a 分类变量, use a bar chart:
```{r}
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut))
```

柱的高就是计数
```{r}
diamonds %>% 
  count(cut)
```

如果变量是连续的，可以使用直方图(histogram)
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)
```

You can compute this by hand by combining dplyr::count() and ggplot2::cut_width():
```{r}
diamonds %>% 
  count(cut_width(carat, 0.5))
```

连续变量的binwith就是区域的大小


```{r}
ggplot(data = diamonds, mapping = aes(x = carat, colour = cut)) +
  geom_freqpoly(binwidth = 0.1)
```

###Typical values 



    1.Which values are the most common? Why?

    2.Which values are rare? Why? Does that match your expectations?

    3.Can you see any unusual patterns? What might explain them?


As an example, the histogram below suggests several interesting questions:

    Why are there more diamonds at whole carats and common fractions of carats?

    Why are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?

    Why are there no diamonds bigger than 3 carats?

```{r}
ggplot(data = diamonds, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.01)
```

对分组变量我们可以这样询问：


    How are the observations within each cluster similar to each other?

    How are the observations in separate clusters different from each other?

    How can you explain or describe the clusters?

    Why might the appearance of clusters be misleading?

控制显示的范围
```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)+coord_cartesian(ylim = c(0, 50))

```

寻找不寻常值：过小的的尺寸或过大的尺寸
```{r}
unusual <- diamonds %>% 
  filter(y < 3 | y > 20) %>% 
  arrange(y)
unusual
```

高价钻石的切割比较
```{r}
ggplot(data = diamonds,mapping = aes(x=price,color=cut)) + geom_freqpoly(binwidth = 50) + 
  coord_cartesian(ylim=c(0,23),xlim = c(15000,19000))
```


###处理异常值
1.直接移除
选择3到20范围内的y
```{r}
diamonds2 <- diamonds %>% 
  filter(between(y, 3, 20))
```
但是不建议，因为损失了其他变量的信息

2.用NA代替异常值
```{r}
diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y))#好操作,向量化赋值
```

ggplot会自动处理NA
```{r}
ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + 
  geom_point(na.rm = TRUE)#如果为F会提示有NA，绘图效果一样
```

如果想要计算一个新变量，如是否取消航班，可用mutate！！！
```{r}
nycflights13::flights %>% 
  mutate(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,#整除
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(sched_dep_time)) + 
    geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4)
```

###相关性
###1.分类和连续数据
加入某一特征作为y轴，绘制分布图
```{r}
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```

There’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price! 
但是分布图不能解释太多东西，这只是画图的第一步。

第二个方法是绘制箱线图
箱体为25%和75%分位点，当超过1.5倍四分位距距时，归为离群点（Q1和Q3的差）
```{r}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_boxplot()
```

我们得到的关于分布的信息变少了，但是获得了更加简洁的图像，更易于比较，
我们又得到了一个违反直觉的现象，切割情况好的反而便宜！如果排序不让你满意，可以使用reorder重新安排顺序

```{r}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) +
  geom_boxplot()

ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))

#变量名字较长，可以使用coord_flip()，旋转坐标
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +
  coord_flip()
```

###2.两个分类数据

计算相应的占比,圈的大小就是计数的大小：
```{r}
ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color))

diamonds %>% 
  count(color, cut)
```

还可以使用热力图：d3heatmap or heatmaply包提供更专业的热力图绘制
```{r}
diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
    geom_tile(mapping = aes(fill = n))#n为用count得出的各类的数值
```

###3.两个连续变量

```{r}
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = carat, y = price))
```

但是散点图随着数据量的增加作用越来越小，因为很多点被覆盖了，你并不知道他的密度。
这时候可以调整透明度来绘制图像。
```{r}
ggplot(data = diamonds) + 
  geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)
```

但是如果数据集再大，透明度的效果会大打折扣，另一种方式是使用
```{r}
library(hexbin)
ggplot(data = diamonds) +
  geom_bin2d(mapping = aes(x = carat, y = price))

ggplot(data = diamonds) +
  geom_hex(mapping = aes(x = carat, y = price))
```

我们还可以融入boxplot
```{r}
ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1))) + coord_cartesian(xlim = c(0,3))
```

在geom_boxplot中，加入varwidth = T可以按照样本的数量决定箱体的宽度，cut\_width(x, width)可以将x分为切分,另一种方法是cut\_number(diamonds$carat, 20)，将分为等数量的区间。
```{r}
ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20)))
```

但是当两者线性关系很明显时，散点图可能会好点：
```{r}
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))
```


##Patterns and models
当模式和系统存在于变量之间，你应该试问自己：
    Could this pattern be due to coincidence (i.e. random chance)?

    How can you describe the relationship implied by the pattern?

    How strong is the relationship implied by the pattern?

    What other variables might affect the relationship?

    Does the relationship change if you look at individual subgroups of the data?

就像火山喷发的数据所示：longer wait times are associated with longer eruptions. The scatterplot also displays the two clusters that we noticed above.
```{r}
ggplot(data = faithful) + 
  geom_point(mapping = aes(x = eruptions, y = waiting))
```

Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it.If two variables covary, you can use the values of one variable to make better predictions about the values of the second.

If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.

Models are a tool for extracting patterns out of data. 

就像我们之前无法理解的切割和价格的关系，这是因为切割水准和克拉数有关而克拉数又和价格有关：我们可以用模型去掉价格和克拉的关系再继续探讨这个问题。我们用克拉拟合价格，然后计算残差，这些残差就是移除了克拉影响后的价格信息（说价格不准确吧）。
```{r}
library(modelr)

mod <- lm(log(price) ~ log(carat), data = diamonds)

diamonds2 <- diamonds %>% 
  add_residuals(mod) %>% 
  mutate(resid = exp(resid))

ggplot(data = diamonds2) + 
  geom_point(mapping = aes(x = carat, y = resid))
```

移除了价格和克拉的线性关系后, 我们可以发现价格和切割的关系变成了: relative to their size, better quality diamonds are more expensive.
```{r}
ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

modelr包的使用在model章节会介绍。

#8.项目的工作流程

1.
强烈推荐关掉退出时工作环境保存,锻炼代码的可重复性。
再加上这两个快捷键：
Ctrl + Shift + F10 to restart RStudio.
Ctrl + Shift + S 将整个草稿运行一遍.


2.projects
File > New Project
~表示当前路径

你生成的数据和结果都可以存放在这
ggsave("diamonds.pdf")
write_csv(diamonds, "diamonds.csv")

退出后，点击.Rproj的文件就能重新打开项目：
Notice you get back to where you left off: it’s the same working directory and command history, and all the files you were working on are still open.

#Tribble数据
```{r}
(tb <- tibble(
  `:)` = "smile", 
  ` ` = "space",
  `2000` = "number"
))
```

```{r}
(tribble(
  ~x, ~y, ~z,
  #--|--|----#这几个杠方便定位
  "a", 2, 3.6,
  "b", 1, 8.5
))
```



```{r}
nycflights13::flights %>% #
  print(n = 10, width = Inf)
```

  
  options(tibble.print_max = n, tibble.print_min = m)
  options(tibble.print_max = n, tibble.print_min = m): if more than m rows, print only n rows. Use        options(dplyr.print_min = Inf) to always show all rows.
  
  Use options(tibble.width = Inf) to always print all columns, regardless of the width of the screen
  
vignette("tibble")#可以让你了解更多信息

```{r}
df <- tibble(
  x = runif(5),
  y = rnorm(5)
)
```
```{r,eval=FALSE}
# Extract by name
df$x
#> [1] 0.434 0.395 0.548 0.762 0.254
df[["x"]]
#> [1] 0.434 0.395 0.548 0.762 0.254
# Extract by position
df[[1]]
#> [1] 0.434 0.395 0.548 0.762 0.254
```

```{r,eval=FALSE}
df %>% .$x
#> [1] 0.434 0.395 0.548 0.762 0.254
df %>% .[["x"]]
#> [1] 0.434 0.395 0.548 0.762 0.254
```

enframe 快速生产tibble
```{r}
enframe(1:3)
```


#readr读取数据
注意：read_csv是dplyr的函数，以tibble格式读取出，读取速度贼快！data.table::fread()这个函数更快，但是和tidyverse不大合适。
注意encoding，在locale = default_locale(encoding="UTF-8")
    read_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter.

    read_fwf() reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions(). read_table() reads a common variation of fixed width files where columns are separated by white space.

    read_log() reads Apache style log files. (But also check out webreadr which is built on top of read_log() and provides many more helpful tools.)
    
利用skip跳过前面n行，用comment跳过特定字符,col_names=F相当于header=F，na为定义那个字符为na的参数
```{r}
read_csv("The first line of metadata
  The second line of metadata
  x,y,z
  1,2,3", skip = 2)

read_csv("# A comment I want to skip\n
# A comment I want to skip\n
  x,y,z
  1,2,3", comment = "#")


read_csv("1,2,3\n4,5,6", col_names = FALSE)
#用col_names加名字
read_csv("1,2,3\n4,5,6", col_names = c("x", "y", "z"))
#定义NA的字符
read_csv("a,b,c\n1,2,.", na = ".")
```

You can also easily adapt what you’ve learned to read tab separated files with read_tsv() and fixed width files with read_fwf()

parse_*()函数，将char格式的数据转换：
```{r}
str(parse_logical(c("TRUE", "FALSE", "NA")))
str(parse_integer(c("1", "2", "3")))
str(parse_date(c("2010-01-01", "1979-10-14")))
parse_integer(c("1", "231", ".", "456"), na = ".")
```

If parsing fails, you’ll get a warning:
```{r,ev}
(x <- parse_integer(c("123", "345", "abc", "123.45")))
```

使用problems(x)可以获得错误信息

处理不和规则的数据格式：“$1000” 、 “10%” 、“1,21”
```{r}
parse_double("1,23", locale = locale(decimal_mark = ","))
parse_number(c("$1000","$243","$823","$1500"))
parse_number(c("1,000","2,243","823","1,500"))
parse_number("It cost $123.45")#parse_number("It cost $123.45,and $231")输出一样，看了只能识别一个
parse_number("$123,456,789")
parse_number("123.456.789", locale = locale(grouping_mark = "."))
parse_number("123'456'789", locale = locale(grouping_mark = "'"))#Used in Switzerland
```

  
  
```{r}
#raw bytes,原始的未编码前模式
charToRaw("Hadley")
#估计编码
x1 <- "\x82\xb1\x82\xf1\x82\xc9\x82\xbf\x82\xcd"
guess_encoding(charToRaw(x1))
parse_character(x1, locale = locale(encoding = "Shift-JIS"))
```

```{r}
#parse_factor()简单的标签赋值，好用
parse_factor(ifelse(rnorm(100)>0,1,0),levels=c(0,1))#然而和as.factor(ifelse(rnorm(100)>0,1,0))有区别吗
```

时间处理
```{r}
parse_datetime("20101010")
parse_datetime("2010-10-01T2010")
parse_date("2010-10-01")
```

```{r}
library(hms)
parse_time("01:10 am")
parse_time("20:10:01")
```

时间数据的格式
```
Year
    %Y (4 digits). 
    %y (2 digits); 00-69 -> 2000-2069, 70-99 -> 1970-1999. 
Month
    %m (2 digits). 
    %b (abbreviated name, like “Jan”). 
    %B (full name, “January”). 
Day
    %d (2 digits). 
    %e (optional leading space). 
Time
    %H 0-23 hour. 
    %I 0-12, must be used with %p. 
    %p AM/PM indicator. 
    %M minutes. 
    %S integer seconds. 
    %OS real seconds. 
    %Z Time zone (as name, e.g. America/Chicago). Beware of abbreviations: if you’re American, note that “EST” is a Canadian time zone that does not have daylight savings time. It is not Eastern Standard Time! We’ll come back to this time zones. 
    %z (as offset from UTC, e.g. +0800). 
Non-digits
    %. skips one non-digit character. 
    %* skips any number of non-digits. 
```
```{r}
parse_date("01/02/15", "%m/%d/%y")
parse_date("01/02/15", "%d/%m/%y")
parse_date("01/02/15", "%y/%m/%d")
parse_date("1 janvier 2015", "%d %B %Y", locale = locale("fr"))
parse_date("January 1, 2010", "%B %d, %Y")
parse_date("2015-Mar-07", "%Y-%b-%d")
parse_date("06-Jun-2017", "%d-%b-%Y")
parse_date(c("August 19 (2015)", "July 1 (2015)"),"%B %d (%Y)")
parse_date("12/30/14","%m/%d/%y") # Dec 30, 2014
parse_date("1705","%y%m")
parse_date("01/02/2010", "%m%.%d%.%Y")
```

See the list of built-in languages in date_names_langs(), or if your language is not already included, create your own with date_names()

csv和csv2的区别
```{r}
read_csv("a,b\n1.0,2.0")
read_csv2("a;b\n1,0;2,0")
```

###readr
readr uses a heuristic to figure out the type of each column: it reads the first 1000 rows and uses some (moderately conservative) heuristics to figure out the type of each column. You can emulate this process with a character vector using guess_parser(), which returns readr’s best guess, and parse_guess() which uses that guess to parse the column:

```{r}
guess_parser("2010-10-01")
guess_parser("15:01")
guess_parser(c("TRUE", "FALSE"))
guess_parser(c("1", "5", "9"))
guess_parser(c("12,352,561"))
str(parse_guess("2010-10-10"))
guess_parser(c("sda"))
```

但是只取1000行有时候也不是很准确我们可以由problem中得到信息。
```{r}
challenge <- read_csv(readr_example("challenge.csv"))
problems(challenge)

challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_integer(),
    y = col_character()
  )
)
```

```{r}
challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_character()
  )
) #integer数据就是4L
tail(challenge)
```


This is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame.
```{r}
df <- tribble(
  ~x,  ~y,
  "1", "1.21",
  "2", "2.32",
  "3", "4.56"
)
df
# Note the column types
type_convert(df)#自动转换格式
```



“Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham

```{r}
# Compute rate per 10,000
table1 %>% 
  mutate(rate = cases / population * 10000)

# Compute cases per year
table1 %>% 
  count(year, wt = cases)


# Visualise changes over time
library(ggplot2)
ggplot(table1, aes(year, cases)) + 
  geom_line(aes(group = country), colour = "grey50") + 
  geom_point(aes(colour = country))
```

most data that you will encounter will be untidy. There are two main reasons:

    Most people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.

    Data is often organised to facilitate some use other than analysis. For example, data is often organised to make entry as easy as possible.

This means for most real analyses, you’ll need to do some tidying. The first step is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems:

    One variable might be spread across multiple columns.

    One observation might be scattered across multiple rows.

Typically a dataset will only suffer from one of these problems; it’ll only suffer from both if you’re really unlucky! To fix these problems, you’ll need the two most important functions in tidyr: gather() and spread().

```{r}

```

